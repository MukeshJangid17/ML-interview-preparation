{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43f4add5-6729-4337-8a76-433f0cbbbc60",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f5782f-9df8-4c83-be8e-5af6721d808f",
   "metadata": {},
   "source": [
    "\n",
    "1. What is linear regression?\n",
    "2. How does linear regression work?\n",
    "3. What are the assumptions of linear regression?\n",
    "4. What are the differences between simple linear regression and multiple linear regression?\n",
    "5. What is the cost function in linear regression?\n",
    "6. How is the cost function minimized in linear regression?\n",
    "7. What is the difference between gradient descent and normal equation in linear regression?\n",
    "8. What is the significance of the intercept term in linear regression?\n",
    "9. What is multicollinearity in linear regression?\n",
    "10. How do you detect multicollinearity in linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe3af0e-cf9b-47ca-94f1-32a72d38cf79",
   "metadata": {},
   "source": [
    "### Q 1 .What is linear regression?\n",
    "Answers :  Linear regression is a statistical method used to model the relationship between a dependent variable (also called the response or target variable) and one or more independent variables (also called predictors or features) by fitting a linear equation to observed data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c60aff-dd58-4f26-8997-9a725553b285",
   "metadata": {},
   "source": [
    "### Q 2.How does linear regression work?\n",
    "Linear regression works by finding the best-fitting linear relationship between the dependent variable and independent variables. This is achieved by minimizing the difference between the predicted values and the actual values, typically using a method like least squares to minimize the sum of the squared residuals (differences between observed and predicted values)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765cf2be-f93a-41bb-b16d-1873c5db42d0",
   "metadata": {},
   "source": [
    "### Q3 . What are the assumptions of linear regression?\n",
    "\n",
    "* Linearity: The relationship between the independent and dependent variable is linear.\n",
    "* Independence: Observations are independent of each other.\n",
    "* Homoscedasticity: The residuals (errors) have constant variance at every level of the independent variable.\n",
    "* Normality: The residuals of the model are normally distributed.\n",
    "* No multicollinearity: Independent variables are not too highly correlated with each other.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b42975-7ade-4b4e-9979-1e3143900719",
   "metadata": {},
   "source": [
    "### Q4.What are the differences between simple linear regression and multiple linear regression?\n",
    "\n",
    "* Simple Linear Regression: Involves one independent variable and one dependent variable. The relationship is modeled with a straight line (y = mx + c).\n",
    "* Multiple Linear Regression: Involves two or more independent variables. The relationship is modeled with a plane or hyperplane (y = b0 + b1x1 + b2x2 + ... + bnxn)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c8c61c-5a49-44bf-a66e-f3af7a4f453d",
   "metadata": {},
   "source": [
    "### Q5. What is the cost function in linear regression?\n",
    "The cost function in linear regression is typically the Mean Squared Error (MSE), which measures the average of the squares of the errors‚Äîthat is, the average squared difference between the estimated values and the actual value "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e892ff45-46a7-4ed9-a7f0-73eebcee0b71",
   "metadata": {},
   "source": [
    "### Q6.How is the cost function minimized in linear regression?\n",
    "The cost function is minimized using optimization techniques like:\n",
    "\n",
    "* Gradient Descent: An iterative optimization algorithm that adjusts the parameters in the direction of the negative gradient of the cost function to find the minimum.\n",
    "* Normal Equation: A closed-form solution that directly computes the parameters by solving a matrix equation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c48710-52d9-499b-821f-e08a2aa15254",
   "metadata": {},
   "source": [
    "### Q7. What is the difference between gradient descent and normal equation in linear regression?\n",
    "\n",
    "* Gradient Descent: An iterative method that updates the model parameters step-by-step to minimize the cost function. It is computationally efficient for large datasets and works well with high-dimensional data.\n",
    "* Normal Equation: A direct method that computes the optimal parameters in one step using matrix operations. It can be computationally expensive for large datasets but does not require iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1681f4e-a49b-4924-8af7-667102ce8556",
   "metadata": {},
   "source": [
    "### Q8. What is the significance of the intercept term in linear regression?\n",
    "The intercept term (often denoted as ùëè0 in multiple regression) represents the expected value of the dependent variable when all independent variables are zero. It is the point where the regression line crosses the y-axis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c06d5db-5935-49f5-9b96-9d8ce07b591b",
   "metadata": {},
   "source": [
    "### Q9. What is multicollinearity in linear regression?\n",
    "Multicollinearity occurs when two or more independent variables in a regression model are highly correlated, meaning they have a strong linear relationship. This can make it difficult to isolate the individual effect of each predictor on the dependent variable, leading to unreliable and unstable estimates of regression coefficients.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2e0e2d-912a-40cc-b196-df97a5bb562c",
   "metadata": {},
   "source": [
    "### Q10. How do you detect multicollinearity in linear regression?\n",
    "\n",
    "* Variance Inflation Factor (VIF): Measures how much the variance of a regression coefficient is inflated due to multicollinearity. VIF values greater than 10 (sometimes 5) suggest significant multicollinearity.\n",
    "* Correlation Matrix: Examines the pairwise correlations between independent variables. High correlation values indicate potential multicollinearity.\n",
    "* Condition Number: A measure derived from the eigenvalues of the predictor matrix. High values indicate multicollinearity.\n",
    "* Tolerance: The reciprocal of VIF; low values indicate high multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2fa06d-e970-484d-bcea-efc4c7b6564f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
